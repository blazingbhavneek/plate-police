{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3152252,"sourceType":"datasetVersion","datasetId":1918159}],"dockerImageVersionId":30018,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os, math, sys\nimport glob, itertools\nimport argparse, random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision.models import vgg19\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.utils import save_image, make_grid\n\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.model_selection import train_test_split\n\nrandom.seed(42)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom IPython.display import clear_output","metadata":{"execution":{"iopub.status.busy":"2023-08-19T04:46:34.053654Z","iopub.execute_input":"2023-08-19T04:46:34.054016Z","iopub.status.idle":"2023-08-19T04:46:34.063937Z","shell.execute_reply.started":"2023-08-19T04:46:34.053981Z","shell.execute_reply":"2023-08-19T04:46:34.062940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_epochs = 2001\ndataset_path = \"/kaggle/input/license-plate-characters-detection-ocr/LP-characters/images/\"\nbatch_size = 8\nlr = 0.0001\n\n# adam: decay of first order momentum of gradient\nb1 = 0.5\n# adam: decay of second order momentum of gradient\nb2 = 0.999\n\n# epoch from which to start lr decay\ndecay_epoch = 100\n\n# number of cpu threads to use during batch generation\nn_cpu = 8\nchannels = 3\n\nos.makedirs(\"images\", exist_ok=True)\nos.makedirs(\"saved_models\", exist_ok=True)\ncuda = torch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T04:25:16.444415Z","iopub.execute_input":"2023-08-19T04:25:16.445159Z","iopub.status.idle":"2023-08-19T04:25:16.541520Z","shell.execute_reply.started":"2023-08-19T04:25:16.445103Z","shell.execute_reply":"2023-08-19T04:25:16.540688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\n\nimage_folder = dataset_path\n\nmin_width = float('inf')\nmin_height = float('inf')\nmax_width = 0\nmax_height = 0\n\nfor filename in os.listdir(image_folder):\n    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n        file_path = os.path.join(image_folder, filename)\n        image = cv2.imread(file_path)\n        height, width, _ = image.shape\n        min_width = min(min_width, width)\n        min_height = min(min_height, height)\n        max_width = max(max_width, width)\n        max_height = max(max_height, height)\n\nprint(\"Minimum image size:\", min_width, \"x\", min_height)\nprint(\"Maximum image size:\", max_width, \"x\", max_height)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T04:25:16.542898Z","iopub.execute_input":"2023-08-19T04:25:16.543266Z","iopub.status.idle":"2023-08-19T04:25:18.681253Z","shell.execute_reply.started":"2023-08-19T04:25:16.543228Z","shell.execute_reply":"2023-08-19T04:25:18.680356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hr_width = 400\nhr_height = 200\n\nlr_height = 50\nlr_width = 100\n\nhr_shape = (hr_width, hr_height)\nlr_shape = (lr_width, lr_height)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T04:25:18.682548Z","iopub.execute_input":"2023-08-19T04:25:18.682932Z","iopub.status.idle":"2023-08-19T04:25:18.690736Z","shell.execute_reply.started":"2023-08-19T04:25:18.682893Z","shell.execute_reply":"2023-08-19T04:25:18.689767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, files, hr_shape, lr_shape):\n        self.hr_shape = hr_shape\n        self.lr_shape = lr_shape\n        self.files = files\n    \n    def __getitem__(self, index):\n        image_hr = cv2.imread(self.files[index % len(self.files)])\n        kernel_size = random.randint(15, 20)\n        angle = random.randint(0, 15)\n        image_hr = cv2.resize(image_hr, self.hr_shape)\n        image = cv2.resize(image_hr, self.lr_shape)\n\n        motion_blur_kernel = np.zeros((kernel_size, kernel_size))\n        motion_blur_kernel[int((kernel_size - 1) / 2), :] = np.ones(kernel_size)\n        rotation_matrix = cv2.getRotationMatrix2D(((kernel_size - 1) / 2, (kernel_size - 1) / 2), angle, 1)\n        motion_blur_kernel = cv2.warpAffine(motion_blur_kernel, rotation_matrix, (kernel_size, kernel_size))\n        motion_blur_kernel = motion_blur_kernel / kernel_size\n\n        image = cv2.filter2D(image, -1, motion_blur_kernel)\n        \n        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        image_hr = Image.fromarray(cv2.cvtColor(image_hr, cv2.COLOR_BGR2RGB))\n        \n        transform = transforms.ToTensor()\n        image = transform(image)\n        image_hr = transform(image_hr)\n        \n        return {\"lr\": image, \"hr\": image_hr}\n\n    def __len__(self):\n        return len(self.files)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T04:25:18.693620Z","iopub.execute_input":"2023-08-19T04:25:18.693988Z","iopub.status.idle":"2023-08-19T04:25:18.713033Z","shell.execute_reply.started":"2023-08-19T04:25:18.693951Z","shell.execute_reply":"2023-08-19T04:25:18.712229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_paths, test_paths = train_test_split(sorted(glob.glob(dataset_path + \"/*.*\")), test_size=0.1, random_state=2002)\ntrain_dataloader = DataLoader(ImageDataset(train_paths, hr_shape=hr_shape, lr_shape = lr_shape), batch_size=batch_size, shuffle=True, num_workers=n_cpu)\ntest_dataloader = DataLoader(ImageDataset(test_paths, hr_shape=hr_shape, lr_shape = lr_shape), batch_size=int(batch_size*0.75), shuffle=False, num_workers=n_cpu)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T04:25:18.715886Z","iopub.execute_input":"2023-08-19T04:25:18.716206Z","iopub.status.idle":"2023-08-19T04:25:18.734001Z","shell.execute_reply.started":"2023-08-19T04:25:18.716135Z","shell.execute_reply":"2023-08-19T04:25:18.733128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n        vgg19_model = vgg19(pretrained=True)\n        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n\n    def forward(self, img):\n        return self.feature_extractor(img)\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(in_features, 0.8),\n            nn.PReLU(),\n            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(in_features, 0.8),\n        )\n\n    def forward(self, x):\n        return x + self.conv_block(x)\n\n\nclass GeneratorResNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=16):\n        super(GeneratorResNet, self).__init__()\n\n        # First layer\n        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4), nn.PReLU())\n\n        # Residual blocks\n        res_blocks = []\n        for _ in range(n_residual_blocks):\n            res_blocks.append(ResidualBlock(64))\n        self.res_blocks = nn.Sequential(*res_blocks)\n\n        # Second conv layer post residual blocks\n        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64, 0.8))\n\n        # Upsampling layers\n        upsampling = []\n        for out_features in range(2):\n            upsampling += [\n                # nn.Upsample(scale_factor=2),\n                nn.Conv2d(64, 256, 3, 1, 1),\n                nn.BatchNorm2d(256),\n                nn.PixelShuffle(upscale_factor=2),\n                nn.PReLU(),\n            ]\n        self.upsampling = nn.Sequential(*upsampling)\n\n        # Final output layer\n        self.conv3 = nn.Sequential(nn.Conv2d(64, out_channels, kernel_size=9, stride=1, padding=4), nn.Tanh())\n\n    def forward(self, x):\n        out1 = self.conv1(x)\n        out = self.res_blocks(out1)\n        out2 = self.conv2(out)\n        out = torch.add(out1, out2)\n        out = self.upsampling(out)\n        out = self.conv3(out)\n        return out\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n\n        self.input_shape = input_shape\n        in_channels, in_height, in_width = self.input_shape\n        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n        self.output_shape = (1, patch_h, patch_w)\n\n        def discriminator_block(in_filters, out_filters, first_block=False):\n            layers = []\n            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n            if not first_block:\n                layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n            layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        layers = []\n        in_filters = in_channels\n        for i, out_filters in enumerate([64, 128, 256, 512]):\n            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n            in_filters = out_filters\n\n        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, img):\n        return self.model(img)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T04:25:18.735978Z","iopub.execute_input":"2023-08-19T04:25:18.736495Z","iopub.status.idle":"2023-08-19T04:25:18.771664Z","shell.execute_reply.started":"2023-08-19T04:25:18.736456Z","shell.execute_reply":"2023-08-19T04:25:18.770562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator = GeneratorResNet()\ndiscriminator = Discriminator(input_shape=(channels, *hr_shape))\nfeature_extractor = FeatureExtractor()\n\nfeature_extractor.eval()\n\ncriterion_GAN = torch.nn.MSELoss()\ncriterion_content = torch.nn.L1Loss()\n\nif cuda:\n    generator = generator.cuda()\n    discriminator = discriminator.cuda()\n    feature_extractor = feature_extractor.cuda()\n    criterion_GAN = criterion_GAN.cuda()\n    criterion_content = criterion_content.cuda()\n\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n\nTensor = torch.cuda.FloatTensor if cuda else torch.Tensor","metadata":{"execution":{"iopub.status.busy":"2023-08-19T04:25:18.773126Z","iopub.execute_input":"2023-08-19T04:25:18.773529Z","iopub.status.idle":"2023-08-19T04:25:33.753013Z","shell.execute_reply.started":"2023-08-19T04:25:18.773490Z","shell.execute_reply":"2023-08-19T04:25:33.752188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_gen_losses, train_disc_losses, train_counter = [], [], []\ntest_gen_losses, test_disc_losses = [], []\ntest_counter = [idx*len(train_dataloader.dataset) for idx in range(1, n_epochs+1)]\n\nfor epoch in range(n_epochs):\n\n    ### Training\n    gen_loss, disc_loss = 0, 0\n    tqdm_bar = tqdm(train_dataloader, desc=f'Training Epoch {epoch} ', total=int(len(train_dataloader)))\n    for batch_idx, imgs in enumerate(tqdm_bar):\n        generator.train(); discriminator.train()\n\n        imgs_lr = Variable(imgs[\"lr\"].type(Tensor))\n        imgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n\n        optimizer_G.zero_grad()\n        gen_hr = generator(imgs_lr)\n    \n        disc_gen_hr = discriminator(gen_hr)\n        \n        valid = Tensor(np.ones(disc_gen_hr.shape))\n        fake = Tensor(np.zeros(disc_gen_hr.shape))\n        \n        loss_GAN = criterion_GAN(disc_gen_hr, valid)\n        \n        # Content loss\n        gen_features = feature_extractor(gen_hr)\n        real_features = feature_extractor(imgs_hr)\n        loss_content = criterion_content(gen_features, real_features.detach())\n        \n        # Total loss\n        loss_G = loss_content + 1e-3 * loss_GAN\n        loss_G.backward()\n        optimizer_G.step()\n\n        ### Train Discriminator\n        optimizer_D.zero_grad()\n        \n        # Loss of real and fake images\n        loss_real = criterion_GAN(discriminator(imgs_hr), valid)\n        loss_fake = criterion_GAN(discriminator(gen_hr.detach()), fake)\n        \n        # Total loss\n        loss_D = (loss_real + loss_fake) / 2\n        loss_D.backward()\n        optimizer_D.step()\n\n        gen_loss += loss_G.item()\n        train_gen_losses.append(loss_G.item())\n        disc_loss += loss_D.item()\n        train_disc_losses.append(loss_D.item())\n        train_counter.append(batch_idx*batch_size + imgs_lr.size(0) + epoch*len(train_dataloader.dataset))\n        tqdm_bar.set_postfix(gen_loss=gen_loss/(batch_idx+1), disc_loss=disc_loss/(batch_idx+1))\n\n    # Testing\n    gen_loss, disc_loss = 0, 0\n    tqdm_bar = tqdm(test_dataloader, desc=f'Testing Epoch {epoch} ', total=int(len(test_dataloader)))\n    for batch_idx, imgs in enumerate(tqdm_bar):\n        generator.eval(); discriminator.eval()\n        \n        # Configure model input\n        imgs_lr = Variable(imgs[\"lr\"].type(Tensor))\n        imgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n        \n        gen_hr = generator(imgs_lr)\n        disc_gen_hr = discriminator(gen_hr)\n        \n        valid = Tensor(np.ones(disc_gen_hr.shape))\n        fake = Tensor(np.zeros(disc_gen_hr.shape))\n        \n        loss_GAN = criterion_GAN(disc_gen_hr, valid)\n\n        # Content loss\n        gen_features = feature_extractor(gen_hr)\n        real_features = feature_extractor(imgs_hr)\n        loss_content = criterion_content(gen_features, real_features.detach())\n        \n        # Total loss\n        loss_G = loss_content + 1e-3 * loss_GAN\n\n        ### Eval Discriminator\n        loss_real = criterion_GAN(discriminator(imgs_hr), valid)\n        loss_fake = criterion_GAN(discriminator(gen_hr.detach()), fake)\n\n        # Total loss\n        loss_D = (loss_real + loss_fake) / 2\n\n        gen_loss += loss_G.item()\n        disc_loss += loss_D.item()\n        tqdm_bar.set_postfix(gen_loss=gen_loss/(batch_idx+1), disc_loss=disc_loss/(batch_idx+1))\n        \n        # Save image grid with upsampled inputs and SRGAN outputs\n        if epoch % 20==0:\n            imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\n            imgs_hr = make_grid(imgs_hr, nrow=1, normalize=True)\n            gen_hr = make_grid(gen_hr, nrow=1, normalize=True)\n            imgs_lr = make_grid(imgs_lr, nrow=1, normalize=True)\n            img_grid = torch.cat((imgs_hr, imgs_lr, gen_hr), -1)\n            save_image(img_grid, f\"images/{str(epoch).zfill(4)}:{str(batch_idx).zfill(2)}.png\", normalize=False)\n\n    test_gen_losses.append(gen_loss/len(test_dataloader))\n    test_disc_losses.append(disc_loss/len(test_dataloader))\n    \n    # Save model checkpoints\n    if np.argmin(test_gen_losses) == len(test_gen_losses)-1:\n        torch.save(generator.state_dict(), \"saved_models/generator.pth\")\n        torch.save(discriminator.state_dict(), \"saved_models/discriminator.pth\")\n    \n    clear_output(wait=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T04:25:33.754445Z","iopub.execute_input":"2023-08-19T04:25:33.754812Z","iopub.status.idle":"2023-08-19T04:33:14.048702Z","shell.execute_reply.started":"2023-08-19T04:25:33.754774Z","shell.execute_reply":"2023-08-19T04:33:14.046823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=train_counter, y=train_gen_losses, mode='lines', name='Train Generator Loss'))\nfig.add_trace(go.Scatter(x=test_counter, y=test_gen_losses, marker_symbol='star-diamond', \n                         marker_color='orange', marker_line_width=1, marker_size=9, mode='markers', name='Test Generator Loss'))\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Train vs. Test Generator Loss\",\n    xaxis_title=\"Number of training examples seen\",\n    yaxis_title=\"Adversarial + Content Loss\"),\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T04:33:17.515274Z","iopub.execute_input":"2023-08-19T04:33:17.515641Z","iopub.status.idle":"2023-08-19T04:33:17.683658Z","shell.execute_reply.started":"2023-08-19T04:33:17.515606Z","shell.execute_reply":"2023-08-19T04:33:17.682760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=train_counter, y=train_disc_losses, mode='lines', name='Train Discriminator Loss'))\nfig.add_trace(go.Scatter(x=test_counter, y=test_disc_losses, marker_symbol='star-diamond', \n                         marker_color='orange', marker_line_width=1, marker_size=9, mode='markers', name='Test Discriminator Loss'))\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Train vs. Test Discriminator Loss\",\n    xaxis_title=\"Number of training examples seen\",\n    yaxis_title=\"Adversarial Loss\"),\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T04:33:20.163471Z","iopub.execute_input":"2023-08-19T04:33:20.163932Z","iopub.status.idle":"2023-08-19T04:33:20.204918Z","shell.execute_reply.started":"2023-08-19T04:33:20.163891Z","shell.execute_reply":"2023-08-19T04:33:20.204077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gif_folder = \"/kaggle/working/gifs/\"\nos.makedirs(gif_folder, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T04:41:17.514979Z","iopub.execute_input":"2023-08-19T04:41:17.515366Z","iopub.status.idle":"2023-08-19T04:41:17.520137Z","shell.execute_reply.started":"2023-08-19T04:41:17.515333Z","shell.execute_reply":"2023-08-19T04:41:17.519188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_folder = \"/kaggle/working/images/\"\n\nimage_files = [filename for filename in os.listdir(image_folder) if filename.lower().endswith(('.png', '.jpg', '.jpeg'))]\n\ndef sort_key(filename):\n    parts = filename.split('.')[0].split(':')\n    epoch = int(parts[0])\n    batch = int(parts[1])\n    return epoch, batch\n\nsorted_image_files = sorted(image_files, key=sort_key)\n\nbatch_arrays = {}\n\nfor filename in sorted_image_files:\n    parts = filename.split('.')[0].split(':')\n    batch = parts[1]\n    \n    if batch not in batch_arrays:\n        batch_arrays[batch] = []\n    \n    batch_arrays[batch].append(filename)\n\nfor batch, filenames in batch_arrays.items():\n    images = []\n    for filename in filenames:\n        file_path = os.path.join(image_folder, filename)\n        image = cv2.imread(file_path)\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        images.append(Image.fromarray(image_rgb))\n    \n    gif_path = os.path.join(gif_folder, f'batch_{batch}.gif')\n    images[0].save(gif_path, save_all=True, append_images=images[1:], loop=0, duration=75)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T04:47:01.825765Z","iopub.execute_input":"2023-08-19T04:47:01.826108Z","iopub.status.idle":"2023-08-19T04:47:10.777504Z","shell.execute_reply.started":"2023-08-19T04:47:01.826075Z","shell.execute_reply":"2023-08-19T04:47:10.776324Z"},"trusted":true},"execution_count":null,"outputs":[]}]}